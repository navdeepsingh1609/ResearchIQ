{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/navdeep/Documents/Hackathon/RollAmp/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import base64\n",
    "from PIL import Image\n",
    "from PyPDF2 import PdfReader\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from pix2text import Pix2Text\n",
    "import fitz  # PyMuPD\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch papers from arXiv API\n",
    "def fetch_arxiv_papers(keyword, max_results=5):\n",
    "    import arxiv  # Ensure arxiv library is installed\n",
    "    search = arxiv.Search(\n",
    "        query=keyword,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    papers = []\n",
    "    for result in search.results():\n",
    "        papers.append({\n",
    "            \"title\": result.title,\n",
    "            \"authors\": [author.name for author in result.authors],\n",
    "            \"summary\": result.summary,\n",
    "            \"pdf_url\": result.pdf_url\n",
    "        })\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clear the downloads folder\n",
    "def clear_downloads_folder(output_dir):\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)  # Remove all files and subdirectories\n",
    "    os.makedirs(output_dir)  # Recreate the folder\n",
    "\n",
    "# Function to download PDF\n",
    "def download_pdf(pdf_url, output_dir=\"downloads\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    pdf_name = pdf_url.split(\"/\")[-1] + \".pdf\"\n",
    "    pdf_path = os.path.join(output_dir, pdf_name)\n",
    "\n",
    "    # Save the PDF locally\n",
    "    response = requests.get(pdf_url)\n",
    "    if response.status_code == 200:\n",
    "        with open(pdf_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        return pdf_path\n",
    "    else:\n",
    "        print(f\"Failed to download {pdf_url} with status code {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text using fitz and Pix2Text\n",
    "def extract_text_from_pdf_with_latex(pdf_file):\n",
    "    math_extractor = Pix2Text()\n",
    "    pdf_document = fitz.open(pdf_file)\n",
    "    extracted_text = \"\"\n",
    "\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document[page_num]\n",
    "        # Extract raw text\n",
    "        text = page.get_text()\n",
    "        extracted_text += f\"Page {page_num + 1} Text:\\n{text}\\n\"\n",
    "\n",
    "        # Extract images for potential LaTeX expressions\n",
    "        for img_index, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            base_image = pdf_document.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            # Extract LaTeX code using Pix2Text\n",
    "            latex_code = math_extractor(image)\n",
    "            extracted_text += f\"\\nMath Expression (Image {img_index + 1}):\\n{latex_code}\\n\"\n",
    "\n",
    "    pdf_document.close()\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process PDF for tables and images using partition_pdf\n",
    "def process_pdf_with_partition(file_path, output_path):\n",
    "    chunks = partition_pdf(\n",
    "        filename=file_path,\n",
    "        infer_table_structure=True,\n",
    "        strategy=\"hi_res\",\n",
    "        extract_image_block_types=[\"Image\",\"Table\"],\n",
    "        image_output_dir_path=output_path,\n",
    "        extract_image_block_to_payload=True,\n",
    "    )\n",
    "\n",
    "    tables = []\n",
    "    images = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if \"Table\" in str(type(chunk)):\n",
    "            tables.append(chunk)\n",
    "        elif \"Image\" in str(type(chunk)):\n",
    "            images.append(chunk.metadata.image_base64)\n",
    "\n",
    "    return tables, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_base64_image(base64_code):\n",
    "    \"\"\"Display a base64-encoded image inline (for Jupyter Notebook or similar).\"\"\"\n",
    "    image_data = base64.b64decode(base64_code)\n",
    "    display(Image(data=image_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "import snowflake.connector as sf\n",
    "from snowflake.snowpark.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import json\n",
    "import snowflake.connector as sf\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Snowflake connection setup\n",
    "def connect_to_snowflake():\n",
    "    return sf.connect(\n",
    "        user=snowflake_user,\n",
    "        password=snowflake_password,\n",
    "        account=snowflake_account,\n",
    "        database=snowflake_database,\n",
    "        warehouse=snowflake_warehouse,\n",
    "        schema=\"public\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk text using RecursiveCharacterTextSplitter\n",
    "def chunk_text_with_langchain(text, chunk_size=500, chunk_overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    return splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING 2025-01-16 21:34:32,093 _showwarnmsg:109] /var/folders/kf/zy5yhv3j6_xd7bjfyt4t2vth0000gn/T/ipykernel_33280/2185667634.py:10: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Impact of Electron-Electron Cusp on Configuration Interaction Energies\n",
      "Saved PDF: downloads/0102536v1.pdf\n",
      "Data collected for paper: Impact of Electron-Electron Cusp on Configuration Interaction Energies\n",
      "Processing: Electron thermal conductivity owing to collisions between degenerate electrons\n",
      "Saved PDF: downloads/0608371v1.pdf\n",
      "Data collected for paper: Electron thermal conductivity owing to collisions between degenerate electrons\n",
      "Processing: Electron pairing: from metastable electron pair to bipolaron\n",
      "Saved PDF: downloads/1802.06593v1.pdf\n",
      "Data collected for paper: Electron pairing: from metastable electron pair to bipolaron\n"
     ]
    }
   ],
   "source": [
    "keyword = \"electron\"\n",
    "max_results = 3\n",
    "output_dir = \"downloads\"\n",
    "\n",
    "# Step 1: Fetch papers from arXiv\n",
    "papers = fetch_arxiv_papers(keyword, max_results)\n",
    "\n",
    "# Step 2: Clear downloads folder\n",
    "clear_downloads_folder(output_dir)\n",
    "\n",
    "# Hierarchical structure for extracted data\n",
    "extracted_data = {}\n",
    "\n",
    "# Step 3: Process papers\n",
    "for paper in papers:\n",
    "    print(f\"Processing: {paper['title']}\")\n",
    "    pdf_path = download_pdf(paper[\"pdf_url\"], output_dir)\n",
    "\n",
    "    if pdf_path:\n",
    "        print(f\"Saved PDF: {pdf_path}\")\n",
    "\n",
    "        # Generate a unique ID for the paper\n",
    "        paper_id = str(uuid.uuid4())\n",
    "\n",
    "        # Initialize storage for this paper\n",
    "        extracted_data[paper_id] = {\n",
    "            \"title\": paper[\"title\"],\n",
    "            \"text_chunks\": [],\n",
    "            \"tables\": [],\n",
    "            \"images\": [],\n",
    "        }\n",
    "\n",
    "        # Extract text and chunk it\n",
    "        text = extract_text_from_pdf_with_latex(pdf_path)\n",
    "        text_chunks = chunk_text_with_langchain(text)\n",
    "        extracted_data[paper_id][\"text_chunks\"].extend(text_chunks)\n",
    "\n",
    "        # Extract tables and images\n",
    "        tables, images = process_pdf_with_partition(pdf_path, output_dir)\n",
    "        extracted_data[paper_id][\"tables\"].extend([table.text for table in tables])\n",
    "        extracted_data[paper_id][\"images\"].extend(images)\n",
    "\n",
    "        print(f\"Data collected for paper: {paper['title']}\")\n",
    "    else:\n",
    "        print(f\"Failed to process {paper['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING 2025-01-16 22:35:15,815 _showwarnmsg:109] /var/folders/kf/zy5yhv3j6_xd7bjfyt4t2vth0000gn/T/ipykernel_33280/943906415.py:4: DeprecationWarning: Complete() is deprecated and will be removed in a future release. Use complete() instead\n",
      "  stream = Complete(\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      " The table presents data on the performance of HF âˆ’ QMC (Hartree-Fock minus Quantum Monte Carlo) calculations for different values of rc (a.u.) and cmin. The table includes the following columns:\n",
      "\n",
      "- **rc (a.u.)**: Values ranging from 3 Ã— 10^âˆ’2 to 0.00.\n",
      "- **cmin**: Values ranging from 1 Ã— 10^âˆ’3 to 0.0.\n",
      "- **HF âˆ’ QMC**: Numerical values corresponding to the calculations.\n",
      "- **Percentages**: Percentage values indicating the relative performance or accuracy of the calculations.\n",
      "\n",
      "The table shows how the HF âˆ’ QMC values and their corresponding percentages change with different combinations of rc and cmin. The percentages range from 11.01% to 100.00%, indicating varying levels of accuracy or performance.\n",
      "\n",
      "This summary is optimized for retrieval by highlighting the key parameters (rc, cmin) and the main data points (HF âˆ’ QMC values and percentages).\n",
      "\n",
      "<class 'str'>\n",
      " The table presents data for HF and QMC methods across different values of rc (1 Ã— 10âˆ’2, 1 Ã— 10âˆ’3, 1 Ã— 10âˆ’4) and cmin (1 Ã— 10âˆ’3, 1 Ã— 10âˆ’4). It includes numerical values and percentages for each combination of rc and cmin. The table shows variations in the results for HF and QMC methods under different conditions.\n",
      "\n",
      "<class 'str'>\n",
      " The table presents data for different values of rc (a.u.) and cmin, comparing HF and QMC methods. It includes absolute values and percentages for each combination of rc and cmin. The percentages are relative to the QMC values, which are set at 100%. The table shows that as rc decreases, the HF values generally increase, and the percentages relative to QMC also increase. The QMC values are consistently higher than the HF values across all rc and cmin combinations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for paper_id, content in extracted_data.items():\n",
    "    #curr pdf\n",
    "    tables=content[\"tables\"]\n",
    "    tableSummary=[]\n",
    "    for table in tables:\n",
    "        prompt = f\"\"\"You are an assistant tasked with summarizing tables for retrieval. \\\n",
    "        These summaries will be embedded and used to retrieve the raw table elements. \\\n",
    "        Give a concise summary of the table that is well optimized for retrieval. {table} \"\"\"\n",
    "        sendToMistral(prompt)\n",
    "        print(\"\\n\")\n",
    "        extracted_data[paper_id][\"tableSummary\"]=tableSummary\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    ''' Getting the base64 string '''\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"multi_modal_rag\",\n",
    "                     embedding_function=OpenAIEmbeddings())\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(text_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "# Add tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [\n",
    "    Document(page_content=s, metadata={id_key: table_ids[i]})\n",
    "    for i, s in enumerate(table_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_tables)\n",
    "retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "# Add image summaries\n",
    "img_ids = [str(uuid.uuid4()) for _ in img_base64_list]\n",
    "summary_img = [\n",
    "    Document(page_content=s, metadata={id_key: img_ids[i]})\n",
    "    for i, s in enumerate(image_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_img)\n",
    "retriever.docstore.mset(list(zip(img_ids, img_base64_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store base64 encoded images\n",
    "img_base64_list = []\n",
    "# Store image summaries\n",
    "image_summaries = []\n",
    "# Prompt : Our prompt here is customized to the type of images we have which is chart in our case\n",
    "prompt = \"Describe the image in detail. Be specific about graphs, such as bar plots.\"\n",
    "# Read images, encode to base64 strings\n",
    "for img_file in sorted(os.listdir(path)):\n",
    "    if img_file.endswith('.jpg'):\n",
    "        img_path = os.path.join(path, img_file)\n",
    "        base64_image = encode_image(img_path)\n",
    "        img_base64_list.append(base64_image)\n",
    "        img_capt = image_captioning(base64_image,prompt)\n",
    "        time.sleep(60)\n",
    "        image_summaries.append(image_captioning(img_capt,prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data into Snowflake\n",
    "def insert_into_snowflake(conn, paper_id, section_type, content, metadata):\n",
    "    metadata_json = json.dumps(metadata)  # Convert dictionary to JSON string\n",
    "    query = f\"\"\"\n",
    "    INSERT INTO parsed_papers (paper_id, section_type, content, metadata)\n",
    "    VALUES (%s, %s, %s, PARSE_JSON(%s))\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query, (paper_id, section_type, content, metadata_json))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from snowflake.cortex import Complete\n",
    " \n",
    "snowflake_config = {\n",
    "    \"account\": snowflake_account,\n",
    "    \"user\": snowflake_user,\n",
    "    \"password\": snowflake_password,\n",
    "    \"role\": snowflake_role,\n",
    "    \"warehouse\": snowflake_warehouse,\n",
    "    \"database\": snowflake_database,\n",
    "    # \"schema\": \"<your_schema>\"\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(snowflake_config).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.cortex import Complete\n",
    "def sendToMistral(prompt):\n",
    "    # session = connect_to_snowflake()\n",
    "    stream = Complete(\n",
    "    \"mistral-large2\",\n",
    "    prompt,\n",
    "    session=session)  \n",
    "    # print(f\"stream {stream} , type {type(stream)}\"  )\n",
    "    for update in stream:\n",
    "        print(update, end = \"\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING 2025-01-16 22:22:33,216 _showwarnmsg:109] /var/folders/kf/zy5yhv3j6_xd7bjfyt4t2vth0000gn/T/ipykernel_33280/943906415.py:4: DeprecationWarning: Complete() is deprecated and will be removed in a future release. Use complete() instead\n",
      "  stream = Complete(\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      " Hello! How can I assist you today? Let's chat about anything you'd like. ðŸ˜Š"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sendToMistral(\"hi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables for retrieval. \\\n",
    "These summaries will be embedded and used to retrieve the raw table elements. \\\n",
    "Give a concise summary of the table that is well optimized for retrieval. Table:{element} \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "002014 (22000): SQL compilation error:\nInvalid expression [PARSE_JSON('{\"title\": \"Impact of Electron-Electron Cusp on Configuration Interaction Energies\"}')] in VALUES clause",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paper_id, content \u001b[38;5;129;01min\u001b[39;00m extracted_data\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Insert text chunks\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m content[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m----> 8\u001b[0m         \u001b[43minsert_into_snowflake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaper_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTEXT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Insert tables\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m content[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtables\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "Cell \u001b[0;32mIn[47], line 9\u001b[0m, in \u001b[0;36minsert_into_snowflake\u001b[0;34m(conn, paper_id, section_type, content, metadata)\u001b[0m\n\u001b[1;32m      4\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124mINSERT INTO parsed_papers (paper_id, section_type, content, metadata)\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mVALUES (%s, %s, %s, PARSE_JSON(%s))\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mcursor() \u001b[38;5;28;01mas\u001b[39;00m cur:\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaper_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msection_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_json\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Hackathon/RollAmp/myenv/lib/python3.9/site-packages/snowflake/connector/cursor.py:1097\u001b[0m, in \u001b[0;36mSnowflakeCursor.execute\u001b[0;34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, _skip_upload_on_content_match, file_stream, num_statements, _dataframe_ast)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     is_integrity_error \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1094\u001b[0m         code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100072\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1095\u001b[0m     )  \u001b[38;5;66;03m# NULL result in a non-nullable column\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m IntegrityError \u001b[38;5;28;01mif\u001b[39;00m is_integrity_error \u001b[38;5;28;01melse\u001b[39;00m ProgrammingError\n\u001b[0;32m-> 1097\u001b[0m     \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Hackathon/RollAmp/myenv/lib/python3.9/site-packages/snowflake/connector/errors.py:284\u001b[0m, in \u001b[0;36mError.errorhandler_wrapper\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21merrorhandler_wrapper\u001b[39m(\n\u001b[1;32m    263\u001b[0m     connection: SnowflakeConnection \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m     error_value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    267\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m        exception to the first handler in that order.\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     handed_over \u001b[38;5;241m=\u001b[39m \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhand_to_other_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handed_over:\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Error\u001b[38;5;241m.\u001b[39merrorhandler_make_exception(\n\u001b[1;32m    292\u001b[0m             error_class,\n\u001b[1;32m    293\u001b[0m             error_value,\n\u001b[1;32m    294\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Hackathon/RollAmp/myenv/lib/python3.9/site-packages/snowflake/connector/errors.py:339\u001b[0m, in \u001b[0;36mError.hand_to_other_handler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mappend((error_class, error_value))\n\u001b[0;32m--> 339\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Hackathon/RollAmp/myenv/lib/python3.9/site-packages/snowflake/connector/errors.py:215\u001b[0m, in \u001b[0;36mError.default_errorhandler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    213\u001b[0m errno \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m done_format_msg \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone_format_msg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error_class(\n\u001b[1;32m    216\u001b[0m     msg\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    217\u001b[0m     errno\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m errno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(errno),\n\u001b[1;32m    218\u001b[0m     sqlstate\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlstate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    219\u001b[0m     sfqid\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    220\u001b[0m     query\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    221\u001b[0m     done_format_msg\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m done_format_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(done_format_msg)\n\u001b[1;32m    223\u001b[0m     ),\n\u001b[1;32m    224\u001b[0m     connection\u001b[38;5;241m=\u001b[39mconnection,\n\u001b[1;32m    225\u001b[0m     cursor\u001b[38;5;241m=\u001b[39mcursor,\n\u001b[1;32m    226\u001b[0m )\n",
      "\u001b[0;31mProgrammingError\u001b[0m: 002014 (22000): SQL compilation error:\nInvalid expression [PARSE_JSON('{\"title\": \"Impact of Electron-Electron Cusp on Configuration Interaction Energies\"}')] in VALUES clause"
     ]
    }
   ],
   "source": [
    "# Step 4: Insert collected data into Snowflake\n",
    "\n",
    "\n",
    "try:\n",
    "    for paper_id, content in extracted_data.items():\n",
    "        # Insert text chunks\n",
    "        for chunk in content[\"text_chunks\"]:\n",
    "            insert_into_snowflake(conn, paper_id, \"TEXT\", chunk, {\"title\": content[\"title\"]})\n",
    "\n",
    "        # Insert tables\n",
    "        # for table in content[\"tables\"]:\n",
    "        #     insert_into_snowflake(conn, paper_id, \"TABLE\", table, {\"title\": content[\"title\"]})\n",
    "\n",
    "        # # Insert images\n",
    "        # for image_path in content[\"images\"]:\n",
    "        #     insert_into_snowflake(conn, paper_id, \"IMAGE\", image_path, {\"title\": content[\"title\"]})\n",
    "\n",
    "        print(f\"Inserted data for paper ID: {paper_id}\")\n",
    "finally:\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the stage and local file path\n",
    "# stage_name = 'research_papers_db.public.fomc'\n",
    "# file_path = './downloads/0608371v1.pdf'\n",
    "\n",
    "# # Upload the PDF to the Snowflake stage\n",
    "# with conn.cursor() as cur:\n",
    "#     cur.execute(f\"PUT file://{file_path} @{stage_name} AUTO_COMPRESS=FALSE\")\n",
    "#     print(\"File uploaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the stage name and file name\n",
    "# stage_name = 'RESEARCH_PAPERS_DB.PUBLIC.FOMC'\n",
    "# file_name = '0608371v1.pdf.gz'  # Use the .gz file name since it's compressed\n",
    "\n",
    "# # SQL query to call the parse_document function\n",
    "# query = f\"\"\"\n",
    "# SELECT TO_VARCHAR(\n",
    "#     SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n",
    "#         '@{stage_name}', \n",
    "#         '{file_name}', \n",
    "#         '{{\"mode\": \"LAYOUT\"}}'\n",
    "#     ):content\n",
    "# ) AS LAYOUT;\n",
    "# \"\"\"\n",
    "\n",
    "# # Execute the query and fetch the result\n",
    "# with conn.cursor() as cur:\n",
    "#     cur.execute(query)\n",
    "#     result = cur.fetchall()\n",
    "#     for row in result:\n",
    "#         print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
